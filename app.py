import time
import os
from langchain.document_loaders import UnstructuredFileLoader
from langchain.embeddings import CacheBackedEmbeddings, OpenAIEmbeddings
from langchain.storage import LocalFileStore
from langchain.text_splitter import CharacterTextSplitter
from langchain.vectorstores.faiss import FAISS
import streamlit as st
from langchain.prompts import ChatPromptTemplate
from langchain.schema.runnable import RunnableLambda, RunnablePassthrough
from langchain.chat_models import ChatOpenAI
from langchain.callbacks.base import BaseCallbackHandler
from langchain.memory import ConversationBufferMemory
from langchain.prompts import MessagesPlaceholder

st.set_page_config(
    page_title="Streamlit is ğŸ”¥",
    page_icon="ğŸ”¥",
    layout="wide",
)

class ChatCallbackHandler(BaseCallbackHandler):

    def __init__(self):
        self.message = ""
        self.message_box = None

    def on_llm_start(self, *args, **kwargs):
        try:
            self.message = ""
            self.message_box = st.empty()
        except Exception:
            pass
        
    def on_llm_end(self, *args, **kwargs):
        try:
            save_message(self.message, "ai")
        except Exception:
            pass

    def on_llm_new_token(self, token: str, **kwargs):
        try:
            self.message += token
            if self.message_box is not None:
                self.message_box.markdown(self.message)
        except Exception:
            pass

# ì‹¤ì œ ê°œë°œ í™˜ê²½ì¸ì§€ í™•ì¸ (í™˜ê²½ë³€ìˆ˜ì— DEV_MODEê°€ ì„¤ì •ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸)
is_actual_dev_mode = os.getenv("DEV_MODE", "false") == "true"

# ì‚¬ì´ë“œë°”ì—ì„œ ëª¨ë¸ê³¼ API í‚¤ ì„¤ì •
with st.sidebar:
    st.write("ì„¤ì •")
    
    # ì‹¤ì œ ê°œë°œ ëª¨ë“œì¼ ë•Œë§Œ í™˜ê²½ë³€ìˆ˜ ì‚¬ìš©
    if is_actual_dev_mode:
        st.success("ê°œë°œ ëª¨ë“œ: í™˜ê²½ë³€ìˆ˜ API í‚¤ ì‚¬ìš©ì¤‘")
        api_key = os.getenv("OPENAI_API_KEY")
        if not api_key:
            st.error("âŒ ê°œë°œ ëª¨ë“œì—ì„œ OPENAI_API_KEY í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤!")
            st.stop()
    else:
        # ë°°í¬ ëª¨ë“œ: ì‚¬ìš©ì ì…ë ¥ ë°›ê¸°
        st.info("API í‚¤ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”")
        api_key = st.text_input(
            "OpenAI API Key",
            type="password",
            placeholder="sk-...",
            help="OpenAI API í‚¤ë¥¼ ì…ë ¥í•˜ì„¸ìš”"
        )

    st.divider()
    
    st.write("íŒŒì¼ ì—…ë¡œë“œ")
    file = st.file_uploader(
        "Upload a .txt .pdf or .docx file",
        type=["pdf", "txt", "docx"],
    )
    
    st.divider()
    
    # ëª¨ë¸ ì„ íƒ
    model_name = st.selectbox(
        "ëª¨ë¸ ì„ íƒ",
        ["gpt-4.1-nano", "gpt-4o-mini", "gpt-4o", "gpt-4-turbo", "gpt-3.5-turbo"],
        index=0,
        help="ì‚¬ìš©í•  OpenAI ëª¨ë¸ì„ ì„ íƒí•˜ì„¸ìš”"
    )
    
    # ì˜¨ë„ ì„¤ì •
    temperature = st.slider(
        "Temperature",
        min_value=0.0,
        max_value=2.0,
        value=0.1,
        step=0.1,
        help="ì‘ë‹µì˜ ì°½ì˜ì„±ì„ ì¡°ì ˆí•©ë‹ˆë‹¤ (ë‚®ì„ìˆ˜ë¡ ì¼ê´€ì„±, ë†’ì„ìˆ˜ë¡ ì°½ì˜ì„±)"
    )
    
    st.divider()
    
   
    
    st.write("ëŒ€í™” ê´€ë¦¬")
    if st.button("ëŒ€í™” ê¸°ë¡ ì´ˆê¸°í™”"):
        try:
            if "memory" in st.session_state:
                st.session_state.memory.clear()
            st.session_state["messages"] = []
            st.success("ëŒ€í™” ê¸°ë¡ì´ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤!")
        except Exception:
            pass
    
    st.divider()
    
    st.write("ì†ŒìŠ¤ì½”ë“œ")
    st.markdown("[GitHub Repository](https://github.com/your-username/full-gpt-challenge)")
    st.markdown("[Streamlit App Code](https://github.com/your-username/full-gpt-challenge/blob/main/home.py)")

# API í‚¤ê°€ ì„¤ì •ë˜ì—ˆëŠ”ì§€ í™•ì¸
if api_key:
    # API í‚¤ê°€ ì…ë ¥ë˜ì—ˆìœ¼ë©´ í™˜ê²½ë³€ìˆ˜ì— ì„¤ì •
    os.environ["OPENAI_API_KEY"] = api_key
    llm = ChatOpenAI(
        model=model_name, 
        temperature=temperature, 
        streaming=True, 
        callbacks=[ChatCallbackHandler()]
    )
else:
    # API í‚¤ê°€ ë¹„ì–´ìˆìœ¼ë©´ í™˜ê²½ë³€ìˆ˜ì—ì„œ ì‚­ì œ
    if "OPENAI_API_KEY" in os.environ:
        del os.environ["OPENAI_API_KEY"]
    
    # ê°„ë‹¨í•œ ì›°ì»´ ë©”ì‹œì§€ í‘œì‹œ
    st.title("ğŸ”¥ DocumentGPT")
    
    st.markdown(
        """
        Welcome! ğŸ”¥
                
        Use this chatbot to ask questions to an AI about your files!
        
        **Please enter your OpenAI API key in the sidebar to get started!**
        """
    )
    
    st.stop()

# ë©”ëª¨ë¦¬ë¥¼ session_stateì— ì €ì¥
if "memory" not in st.session_state:
    try:
        st.session_state.memory = ConversationBufferMemory(
            llm=llm,
            max_token_limit=120,
            return_messages=True,
            memory_key="history"
        )
    except Exception:
        pass

# messagesë„ session_stateì— ì €ì¥
if "messages" not in st.session_state:
    st.session_state["messages"] = []

# memory ë³€ìˆ˜ ì•ˆì „í•˜ê²Œ í• ë‹¹
try:
    memory = st.session_state.memory
except Exception:
    memory = ConversationBufferMemory(
        llm=llm,
        max_token_limit=120,
        return_messages=True,
        memory_key="history"
    )
    st.session_state.memory = memory

@st.cache_data(show_spinner="Embedding file..." )
def load_and_split(file):
    file_content = file.read()
    file_path = f"./.cache/files/{file.name}"
    with open(file_path, "wb") as f:
        f.write(file_content)
    splitter = CharacterTextSplitter.from_tiktoken_encoder(
        separator="\n", chunk_size=600, chunk_overlap=100
    )
    loader = UnstructuredFileLoader(file_path)
    docs = loader.load_and_split(text_splitter=splitter)
    return docs

def embed_and_retrieve(docs, file):
    cache_dir = LocalFileStore(f"./.cache/embeddings/{file.name}")
    embeddings = OpenAIEmbeddings()
    cached_embeddings = CacheBackedEmbeddings.from_bytes_store(embeddings, cache_dir)
    vectorstore = FAISS.from_documents(docs, cached_embeddings)
    return vectorstore.as_retriever()

st.title("DocumentGPT")

st.markdown(
    """
Welcome!
            
Use this chatbot to ask questions to an AI about your files!

Upload a file on the sidebar to get started!
"""
)

def send_message(message, role, save=True):
    with st.chat_message(role):
        st.markdown(message)
    if save:
        save_message(message, role)

def save_message(message, role):
    st.session_state["messages"].append({"role": role, "message": message})

def paint_history():
    for message in st.session_state["messages"]:
        send_message(message["message"], message["role"], save=False)

prompt = ChatPromptTemplate.from_messages([
    ("system", """You are a helpful assistant that can answer questions about documents and previous conversations. 

Use both the document context and conversation history to provide accurate answers. If the user asks about something we discussed before, refer to that information.
If you don't know the answer just say you don't know, don't make it up:
Document context: {context}
"""),
    MessagesPlaceholder(variable_name="history"),
    ("user", "{question}"),
])

def docs_to_context(docs):
    return "\n\n".join([doc.page_content for doc in docs])

if file:
    retriever = embed_and_retrieve(load_and_split(file), file)

    if retriever:
        st.success("ğŸ‰ íŒŒì¼ì´ ì„±ê³µì ìœ¼ë¡œ ì²˜ë¦¬ë˜ì—ˆìŠµë‹ˆë‹¤!")
        send_message("I'm ready to answer your questions!", "ai", save=False)
        paint_history()

        def ask(question):
            try:
                memory_vars = st.session_state.memory.load_memory_variables({})
                history = memory_vars.get("history", [])
            except Exception:
                history = []
            
            docs = retriever.invoke(question)
            context = docs_to_context(docs)
            
            result = prompt.invoke({
                "question": question, 
                "context": context,
                "history": history
            })
            
            response = llm.invoke(result)
            
            try:
                st.session_state.memory.save_context(
                    {"input": question}, 
                    {"output": response.content}
                )
            except Exception:
                pass
            
            return response.content

        message = st.chat_input("Ask me anything!")
        if message:
            send_message(message, "human")
            with st.chat_message("ai"):
                response = ask(message)
    else:
        st.warning("íŒŒì¼ ì²˜ë¦¬ë¥¼ ì™„ë£Œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ìœ„ì˜ ì˜¤ë¥˜ ë©”ì‹œì§€ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")
else:
    st.session_state["messages"] = []


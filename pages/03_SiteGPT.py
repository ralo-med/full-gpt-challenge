import streamlit as st
from langchain.prompts import ChatPromptTemplate
from langchain.document_loaders import SitemapLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.vectorstores.faiss import FAISS
from langchain.embeddings import OpenAIEmbeddings
from langchain.schema.runnable import RunnablePassthrough, RunnableLambda
from langchain_openai import ChatOpenAI
from langchain.callbacks.base import BaseCallbackHandler
from langchain.document_transformers import Html2TextTransformer
from utils import setup_sidebar, save_settings_to_session
import os
import re


class ChatCallbackHandler(BaseCallbackHandler):
    def __init__(self, container):
        self.container = container
        self.message = ""

    def on_llm_new_token(self, token: str, **kwargs) -> None:
        self.message += token
        self.container.markdown(self.message)


st.set_page_config(
    page_title="SiteGPT",
    page_icon="ğŸ¤–",
)

answers_prompt = ChatPromptTemplate.from_template(
    """
    Using ONLY the following context answer the user's question. If you can't just say you don't know, don't make anything up.
                                                  
    Then, give a score to the answer between 0 and 5 (ex:4.7).

    If the answer answers the user question the score should be high, else it should be low.

    Make sure to always include the answer's score even if it's 0.

    Context: {context}
                                                  
    Examples:
                                                  
    Question: How far away is the moon?
    Answer: The moon is 384,400 km away.
    Score: 4.8
                                                  
    Question: How far away is the sun?
    Answer: I don't know
    Score: 0.0
                                                  
    Your turn!

    Question: {question}
"""
)

choose_prompt = ChatPromptTemplate.from_messages(
    [
        (
            "system",
            """
            Use ONLY the following pre-existing answers to answer the user's question.

            Use the answers that have the highest score (more helpful) and favor the most recent ones.

            Cite sources and return the sources of the answers as they are, do not change them.

            Answers: {answers}
            """,
        ),
        ("human", "{question}"),
    ]
)

html2text_transformer = Html2TextTransformer()


def parse_page(soup):
    header = soup.find("header")
    footer = soup.find("footer")
    if header:
        header.decompose()
    if footer:
        footer.decompose()
    return str(soup.get_text()).replace("\n", " ").replace("\xa0", " ")


def extract_score(text):
    match = re.search(r"Score:\s*([0-5](?:\.\d+)?)", text)
    return float(match.group(1)) if match else 0.0


def get_answer(inputs):
    llm = inputs["llm"]
    question = inputs["question"]
    docs = inputs["docs"]

    answer_chain = answers_prompt | llm
    answers = []

    for doc in docs:
        try:
            content = doc.page_content
            result = answer_chain.invoke({"context": content, "question": question})
            score = extract_score(result.content)
            answers.append(
                {
                    "answer": result.content,
                    "source": doc.metadata.get("source", "Unknown"),
                    "date": doc.metadata.get("lastmod", "Unknown"),
                    "score": score,
                }
            )
        except Exception as e:
            st.warning(f"ë¬¸ì„œ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            continue

    return {"question": question, "answer": answers, "llm": llm}


def choose_answer(inputs):
    llm = inputs["llm"]
    question = inputs["question"]
    answers = inputs["answer"]

    choose_chain = choose_prompt | llm
    condensed_answer = "\n\n".join(
        f"{answer['answer']}\nSource: {answer['source']}\nDate: {answer['date']}\nScore: {answer['score']}\n\n"
        for answer in answers
    )
    return choose_chain.invoke({"answers": condensed_answer, "question": question})


@st.cache_resource(show_spinner="Loading sitemap...", ttl=3600)
def load_sitemap_docs(url):
    """ì‚¬ì´íŠ¸ë§µì„ ë¡œë“œí•˜ê³  ë¬¸ì„œë¥¼ ë¶„í• í•©ë‹ˆë‹¤. API í‚¤ê°€ í•„ìš”í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤."""
    import urllib3
    import requests
    from requests.adapters import HTTPAdapter

    urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

    with requests.Session() as session:
        session.verify = False
        adapter = HTTPAdapter()
        session.mount("http://", adapter)
        session.mount("https://", adapter)
        
        text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(
            chunk_size=500, chunk_overlap=100
        )

        loader = SitemapLoader(
            url,
            filter_urls=[
                r"^(.*\/ai-gateway\/).*",
                r"^(.*\/vectorize\/).*",
                r"^(.*\/workers-ai\/).*",
            ],
            parsing_function=parse_page,
            session=session,
        )
        loader.requests_per_second = 10  # ì†ë„ ì¡°ì ˆ
        docs = loader.load()

    split_docs = text_splitter.split_documents(docs)
    return split_docs


def create_retriever(docs):
    """ë¶„í• ëœ ë¬¸ì„œë¡œë¶€í„° ë²¡í„° ìŠ¤í† ì–´ì™€ ë¦¬íŠ¸ë¦¬ë²„ë¥¼ ìƒì„±í•©ë‹ˆë‹¤. API í‚¤ê°€ í•„ìš”í•©ë‹ˆë‹¤."""
    try:
        vectorstore = FAISS.from_documents(docs, OpenAIEmbeddings())
        return vectorstore.as_retriever()
    except Exception as e:
        if "api_key" in str(e).lower() or "openai_api_key" in str(e).lower():
            st.error("âŒ OpenAI API í‚¤ê°€ í•„ìš”í•©ë‹ˆë‹¤. ì‚¬ì´ë“œë°”ì—ì„œ API í‚¤ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
            st.info("ğŸ’¡ ì‚¬ì´íŠ¸ë§µì€ ë¡œë“œë˜ì—ˆì§€ë§Œ, ì„ë² ë”© ìƒì„±ì„ ìœ„í•´ API í‚¤ê°€ í•„ìš”í•©ë‹ˆë‹¤.")
        else:
            st.error(f"âŒ ì„ë² ë”© ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")
        return None


st.title("SiteGPT")
st.write("Cloudflare AI ì œí’ˆ ë¬¸ì„œ ì§ˆì˜ì‘ë‹µ")


st.markdown(
    """
ì´ ì‹œìŠ¤í…œì€ Cloudflareì˜ AI Gateway, Vectorize, Workers AI ì œí’ˆì— ëŒ€í•œ ì§ˆë¬¸ì„ ë‹µë³€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:
- **AI Gateway**: AI ëª¨ë¸ API í†µí•© ë° ê´€ë¦¬
- **Vectorize**: ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ë° ì„ë² ë”© ì„œë¹„ìŠ¤  
- **Workers AI**: ì„œë²„ë¦¬ìŠ¤ AI ì¶”ë¡  ì„œë¹„ìŠ¤

Cloudflare ê³µì‹ ë¬¸ì„œê°€ ìë™ìœ¼ë¡œ ë¡œë“œë©ë‹ˆë‹¤.
"""
)

with st.sidebar:
    api_key, model_name, temperature = setup_sidebar()
    if api_key:
        save_settings_to_session(api_key, model_name, temperature)
        os.environ["OPENAI_API_KEY"] = api_key

if not api_key:
    st.info("Please enter your OpenAI API key to proceed.")
    st.stop()

llm = ChatOpenAI(model_name=model_name, temperature=temperature)
url = "https://developers.cloudflare.com/sitemap.xml"

if url:
    if url.endswith(".xml"):
        if st.button("ğŸ”„ ë‹¤ì‹œ ë¡œë“œ", key="reload_button"):
            st.cache_resource.clear()
            st.success("ìºì‹œê°€ ì§€ì›Œì¡ŒìŠµë‹ˆë‹¤. í˜ì´ì§€ë¥¼ ìƒˆë¡œê³ ì¹¨í•˜ì„¸ìš”.")

        try:
            # 1ë‹¨ê³„: ì‚¬ì´íŠ¸ë§µ ë¡œë”© (API í‚¤ ë¶ˆí•„ìš”)
            docs = load_sitemap_docs(url)
            st.success("âœ… ì‚¬ì´íŠ¸ë§µ ë¡œë”© ì™„ë£Œ!")

            # 2ë‹¨ê³„: ì„ë² ë”© ìƒì„± (API í‚¤ í•„ìš”)
            retriever = create_retriever(docs)

            if retriever:
                query = st.text_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”:", key="query")
                if query:
                    if llm is None:
                        st.error("API í‚¤ë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”.")
                    else:
                        # ë¬¸ì„œ ê²€ìƒ‰ - ëª¨ë“  ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰
                        retrieved_docs = retriever.invoke(query)

                        chain = (
                            {
                                "docs": lambda x: retrieved_docs,
                                "question": RunnablePassthrough(),
                                "llm": lambda x: llm,
                            }
                            | RunnableLambda(get_answer)
                            | RunnableLambda(choose_answer)
                        )
                        result = chain.invoke(query)
                        st.write("**ë‹µë³€:**")
                        st.write(result.content)
            else:
                st.warning("âš ï¸ ì„ë² ë”©ì´ ìƒì„±ë˜ì§€ ì•Šì•„ ì§ˆì˜ì‘ë‹µì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                st.info("ğŸ’¡ ì‚¬ì´ë“œë°”ì—ì„œ OpenAI API í‚¤ë¥¼ ì…ë ¥í•œ í›„ í˜ì´ì§€ë¥¼ ìƒˆë¡œê³ ì¹¨í•˜ì„¸ìš”.")

        except Exception as e:
            st.error(f"âŒ ì‚¬ì´íŠ¸ë§µ ë¡œë”© ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")
            st.info("ğŸ’¡ íŒ: ë„¤íŠ¸ì›Œí¬ ì—°ê²°ì„ í™•ì¸í•˜ê±°ë‚˜ ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ë³´ì„¸ìš”.")
    else:
        st.error("Please enter a Sitemap URL.")
else:
    st.markdown(
        """# Cloudflare AI ì œí’ˆ ë¬¸ì„œ
                 
ì´ ì‹œìŠ¤í…œì€ Cloudflareì˜ AI Gateway, Vectorize, Workers AI ì œí’ˆì— ëŒ€í•œ ì§ˆë¬¸ì„ ë‹µë³€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:
- **AI Gateway**: AI ëª¨ë¸ API í†µí•© ë° ê´€ë¦¬
- **Vectorize**: ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ë° ì„ë² ë”© ì„œë¹„ìŠ¤  
- **Workers AI**: ì„œë²„ë¦¬ìŠ¤ AI ì¶”ë¡  ì„œë¹„ìŠ¤

Cloudflare ê³µì‹ ë¬¸ì„œê°€ ë¡œë“œë˜ì–´ ìˆìŠµë‹ˆë‹¤.
                 
"""
    )






#!/usr/bin/env python3
import os, json, warnings, time
from typing import Dict, List, Any, Tuple

import streamlit as st
from openai import OpenAI, InternalServerError
from requests.exceptions import RequestException

from langchain_community.utilities.duckduckgo_search import DuckDuckGoSearchAPIWrapper
from langchain_community.utilities import WikipediaAPIWrapper
from langchain_community.document_loaders import WebBaseLoader

from utils import setup_sidebar, save_settings_to_session

warnings.filterwarnings(
    "ignore",
    message=r"This package \(`duckduckgo_search`\) has been renamed to `ddgs`!",
    category=RuntimeWarning,
)

os.environ.setdefault(
    "USER_AGENT",
    (
        "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) "
        "AppleWebKit/537.36 (KHTML, like Gecko) Chrome/125.0 Safari/537.36"
    ),
)


def wiki_search(inputs: Dict[str, Any]) -> str:
    return WikipediaAPIWrapper().run(inputs["query"])

def ddg_search(inputs: Dict[str, Any]) -> str:
    wrapper = DuckDuckGoSearchAPIWrapper(safesearch="off")
    return wrapper.run(inputs["query"])

def fetch_webpage(inputs: Dict[str, Any]) -> str:
    url = inputs["url"]
    try:
        loader = WebBaseLoader(
            url,
            requests_kwargs={
                "timeout": 15,
                "headers": {"User-Agent": os.environ["USER_AGENT"]},
            },
        )
        docs = loader.load()
    except RequestException as e:
        return f"[SKIPPED] {url} ({e.__class__.__name__}: {e})"
    content = "\n\n".join(doc.page_content for doc in docs)
    cleaned = "\n".join(line.strip() for line in content.splitlines() if line.strip())
    return cleaned if cleaned else f"[EMPTY] {url}"

def save_to_file(inputs: Dict[str, Any]) -> str:
    try:
        content = inputs.get("content", "")
        if not content:
            return "ì˜¤ë¥˜: ì €ì¥í•  ë‚´ìš©ì´ ì—†ìŠµë‹ˆë‹¤."
        with open("result.txt", "w", encoding="utf-8") as f:
            f.write(content)
        if os.path.exists("result.txt"):
            file_size = os.path.getsize("result.txt")
            return f"íŒŒì¼ ì €ì¥ ì™„ë£Œ: result.txt ({file_size} ë°”ì´íŠ¸)"
        else:
            return "ì˜¤ë¥˜: íŒŒì¼ì´ ìƒì„±ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
    except Exception as e:
        return f"íŒŒì¼ ì €ì¥ ì˜¤ë¥˜: {str(e)}"

functions_map = {
    "wiki_search": wiki_search,
    "ddg_search": ddg_search,
    "fetch_webpage": fetch_webpage,
    "save_to_file": save_to_file,
}


tools = [
    {
        "name": "wiki_search",
        "type": "function",
        "description": "Return a concise summary from Wikipedia for the query.",
        "parameters": {
            "type": "object",
            "properties": {"query": {"type": "string"}},
            "required": ["query"],
            "additionalProperties": False,
        },
    },
    {
        "name": "ddg_search",
        "type": "function",
        "description": "Return DuckDuckGo text search results for the query.",
        "parameters": {
            "type": "object",
            "properties": {"query": {"type": "string"}},
            "required": ["query"],
            "additionalProperties": False,
        },
    },
    {
        "name": "fetch_webpage",
        "type": "function",
        "description": "Download and return raw textual content from a webpage URL.",
        "parameters": {
            "type": "object",
            "properties": {"url": {"type": "string", "format": "uri"}},
            "required": ["url"],
            "additionalProperties": False,
        },
    },
    {
        "name": "save_to_file",
        "type": "function",
        "description": "Save provided text to result.txt on disk.",
        "parameters": {
            "type": "object",
            "properties": {"content": {"type": "string"}},
            "required": ["content"],
            "additionalProperties": False,
        },
    },
]

SYSTEM_PROMPT = (
    "You are an advanced research assistant. Your primary goal is to produce a high-quality, comprehensive research report based on the user's query.\n\n"
    "## General Workflow:\n"
    "Your standard process is a 4-step sequence: `wiki_search` -> `ddg_search` -> `fetch_webpage` -> `save_to_file`.\n\n"
    "## CRITICAL INSTRUCTIONS:\n"
    "1. **REACT TO FAILURES (Recovery):** This is your most important instruction. If a tool fails or provides poor results (e.g., `fetch_webpage` returns `[SKIPPED]` or `[EMPTY]`), you MUST NOT proceed blindly. You MUST attempt to recover. For example, use `ddg_search` again to find a new, more reliable URL, then try `fetch_webpage` on that new URL. Your goal is a complete report; do not give up easily.\n"
    "2. **USER COMMANDS FIRST:** If the user gives a direct command (e.g., 'save now', 'search for this'), that command overrides the general workflow. Execute it immediately.\n"
    "3. **NO PREMATURE SUMMARIES:** Do not provide any text summary or answer to the user until you have successfully called `save_to_file`. Your only outputs should be tool calls until the final step.\n"
    "4. **MANDATORY SAVE & COMPREHENSIVE CONTENT:** The `save_to_file` function is the MANDATORY final step. The 'content' for this function must be a detailed, multi-paragraph report synthesizing ALL information you have gathered.\n"
)


def safe_create_responses(client: OpenAI, **kwargs):
    delay = 1.0
    for attempt in range(3):
        try:
            return client.responses.create(**kwargs)
        except InternalServerError:
            if attempt == 2:
                raise
            time.sleep(delay)
            delay *= 2


def render_progress_steps(progress_steps: List[Any], placeholder=None):
    """ì§„í–‰ ë‹¨ê³„ë“¤ì„ UIì— ë Œë”ë§í•˜ëŠ” í•¨ìˆ˜"""
    container = placeholder.container() if placeholder else st
    for step in progress_steps:
        if isinstance(step, dict) and step.get("type") == "result_dropdown":
            with container.expander(f"ğŸ“„ ë‹¨ê³„ {step['step']} ê²°ê³¼ - {step['tool_name']} ({step['tool']})"):
                st.write(f"**ì‚¬ìš© ë„êµ¬**: {step['tool']}")
                st.write(f"**ë„êµ¬ ì„¤ëª…**: {step['tool_name']}")
                st.write("**ê²°ê³¼**:")
                st.text(step['result'])
        elif "ğŸ” **ë‹¨ê³„" in step:
            container.info(step)
        elif "âœ… **ë‹¨ê³„" in step:
            container.success(step)
        elif "ğŸ‰ **ì—°êµ¬ ì™„ë£Œ**" in step:
            container.success(step)
        elif "ğŸ¤” AI assistant" in step:
            container.info(step)
        else:
            container.write(step)


def research_assistant_streaming(
    client: OpenAI,
    user_msg: str,
    history: List[Dict[str, Any]],
    progress_placeholder,
    model_name: str,
) -> Tuple[List[Dict[str, Any]], str]:
    if not history or history[-1].get("content") != user_msg or history[-1].get("role") != "user":
        history.append({"role": "user", "content": user_msg})

    st.session_state.progress_steps = []
    progress_steps = st.session_state.progress_steps
    final_summary = ""

    progress_steps.append("ğŸ¤” AI assistant ë¶„ì„ ì‹œì‘...")
    render_progress_steps(progress_steps, progress_placeholder)

    step_count = 0
    while True: 
        step_count += 1
        resp = safe_create_responses(
            client,
            model=model_name,
            input=history,
            tools=tools,
            temperature=st.session_state.temperature
        )

        response_message = resp.output[0] if resp.output else None
        
        if not response_message or response_message.type != "function_call":
            is_save_done = any(
                isinstance(h, dict) and h.get("name") == "save_to_file"
                for h in history
            )
            # ì €ì¥ì´ ì™„ë£Œëœ í›„ì˜ í…ìŠ¤íŠ¸ ì‘ë‹µì€ ìµœì¢… ë‹µë³€ìœ¼ë¡œ ê°„ì£¼í•˜ê³  ë£¨í”„ ì¢…ë£Œ
            if is_save_done:
                break
            
            # ì €ì¥ì´ ì™„ë£Œë˜ì§€ ì•Šì•˜ëŠ”ë° í…ìŠ¤íŠ¸ë¥¼ ìƒì„±í•˜ë©´, ê·œì¹™ì„ ë‹¤ì‹œ ì•Œë ¤ì£¼ê³  ê³„ì† ì§„í–‰í•˜ë„ë¡ ìœ ë„
            premature_text = response_message.text.strip() if response_message and hasattr(response_message, "text") else ""
            if premature_text:
                progress_steps.append(f"âš ï¸ AIê°€ ì¤‘ê°„ ìš”ì•½ì„ ì‹œë„í–ˆìŠµë‹ˆë‹¤. ê·œì¹™ì— ë”°ë¼ ë‹¤ìŒ ë„êµ¬ë¥¼ ì‚¬ìš©í•˜ë„ë¡ ì§€ì‹œí•©ë‹ˆë‹¤.")
                render_progress_steps(progress_steps, progress_placeholder)
                history.append({"role": "assistant", "content": premature_text})
            
            history.append({
                "role": "user", 
                "content": "You must not generate a text response yet. You must call a tool. Either continue researching with another tool or, if you have sufficient information, call `save_to_file` to create the final report."
            })
            continue # ë‹¤ìŒ ë£¨í”„ë¥¼ ëŒë©° AIê°€ ë„êµ¬ë¥¼ ì‚¬ìš©í•˜ë„ë¡ ê°•ì œ

        call = response_message
        tool_name = call.name
        tool_descriptions = {
            "wiki_search": "ğŸ“š ìœ„í‚¤ë°±ê³¼ ê²€ìƒ‰", "ddg_search": "ğŸ” DuckDuckGo ì›¹ ê²€ìƒ‰", 
            "fetch_webpage": "ğŸŒ ì›¹í˜ì´ì§€ ë‚´ìš© ì¶”ì¶œ", "save_to_file": "ğŸ’¾ ê²°ê³¼ë¥¼ íŒŒì¼ë¡œ ì €ì¥"
        }
        tool_ui_name = tool_descriptions.get(tool_name, 'ë„êµ¬ ì‹¤í–‰ ì¤‘')
        
        progress_steps.append(f"ğŸ” **ë‹¨ê³„ {step_count}** {tool_ui_name} ì¤‘...")
        render_progress_steps(progress_steps, progress_placeholder)
        
        result = functions_map[tool_name](json.loads(call.arguments))
        
        history.extend([call, {"type": "function_call_output", "call_id": call.call_id, "output": result}])
        
        progress_steps.append(f"âœ… **ë‹¨ê³„ {step_count} ì™„ë£Œ**: {tool_ui_name}")
        progress_steps.append({"type": "result_dropdown", "step": step_count, "tool": tool_name, "tool_name": tool_ui_name, "result": result})
        render_progress_steps(progress_steps, progress_placeholder)

        if tool_name == "save_to_file":
            final_summary = json.loads(call.arguments).get("content", "")
            progress_steps.append("ğŸ‰ **ì—°êµ¬ ì™„ë£Œ**: íŒŒì¼ ì €ì¥ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
            render_progress_steps(progress_steps, progress_placeholder)
            # AIì˜ ì¶”ê°€ì ì¸ ë‹µë³€ ìƒì„± ëŒ€ì‹ , ì €ì¥ëœ ìš”ì•½ì„ ìµœì¢… ë‹µë³€ìœ¼ë¡œ ì„¤ì •
            response_message = f"ğŸ¯ [ìµœì¢… ì—°êµ¬ ìš”ì•½]\n{final_summary}" if final_summary else "ì—°êµ¬ë¥¼ ì™„ë£Œí–ˆì§€ë§Œ, ìš”ì•½ì´ ìƒì„±ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
            break

        progress_steps.append("ğŸ¤” AI assistant ë‹¤ìŒ ë‹¨ê³„ ë¶„ì„ ì¤‘...")
        render_progress_steps(progress_steps, progress_placeholder)

    # ìµœì¢… ë‹µë³€ ìƒì„±
    if isinstance(response_message, str):
        final_answer = response_message
    elif hasattr(response_message, 'text') and response_message.text:
        final_answer = response_message.text.strip()
    else:
        final_answer = f"ğŸ¯ [ìµœì¢… ì—°êµ¬ ìš”ì•½]\n{final_summary}" if final_summary else "ì—°êµ¬ë¥¼ ì™„ë£Œí–ˆì§€ë§Œ, ìš”ì•½ì´ ìƒì„±ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
        
    return history, final_answer


st.set_page_config(
    page_title="ResearchGPT",
    page_icon="ğŸ”",
    layout="wide",
)

st.title("ResearchGPT ğŸ”")
st.markdown("""
ì›¹ ê²€ìƒ‰ê³¼ ë„êµ¬ë¥¼ ì‚¬ìš©í•˜ì—¬ ì§ˆë¬¸ì— ë‹µë³€í•˜ëŠ” AI ì—°êµ¬ ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.

**ê¸°ëŠ¥:**
- ğŸ“š ìœ„í‚¤ë°±ê³¼ ê²€ìƒ‰
- ğŸ” DuckDuckGo ì›¹ ê²€ìƒ‰
- ğŸŒ ì›¹í˜ì´ì§€ ë‚´ìš© ì¶”ì¶œ
- ğŸ’¾ ê²°ê³¼ë¥¼ íŒŒì¼ë¡œ ì €ì¥
- â­ï¸ ì €ì¥ì´ ì´ë£¨ì–´ ì§€ì§€ ì•Šì„ì‹œ ë¶„ì„ì´ ì™„ë£Œ ëœê²ƒì´ ì•„ë‹ˆë‹ˆ ê³„ì† ëŒ€í™”ë¥¼ ì´ì–´ê°€ê±°ë‚˜ ì €ì¥í•´ ë‹¬ë¼ í•˜ì„¸ìš”!

""")

with st.sidebar:
    st.header("ì„¤ì •")
    api_key, model_name, temperature = setup_sidebar()
    if api_key:
        save_settings_to_session(api_key, model_name, temperature)
        os.environ["OPENAI_API_KEY"] = api_key
    else:
        st.info("ì‚¬ì´ë“œë°”ì—ì„œ OpenAI API í‚¤ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
        st.stop()


    st.header("ğŸ“ ì €ì¥ëœ íŒŒì¼")
    if os.path.exists("result.txt"):
        try:
            with open("result.txt", "r", encoding="utf-8") as f:
                file_content = f.read()
            st.success("âœ… result.txt ì €ì¥ ì™„ë£Œ")
            st.write(f"**íŒŒì¼ í¬ê¸°:** {len(file_content)} ë¬¸ì")
            st.download_button(
                label="ğŸ“¥ result.txt ë‹¤ìš´ë¡œë“œ",
                data=file_content,
                file_name="result.txt",
                mime="text/plain"
            )
            if st.button("ğŸ—‘ï¸ íŒŒì¼ ì‚­ì œ", disabled=st.session_state.get("processing", False)):
                os.remove("result.txt")
                st.success("íŒŒì¼ì´ ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤!")
                st.rerun()
        except Exception as e:
            st.error(f"íŒŒì¼ ì½ê¸° ì˜¤ë¥˜: {str(e)}")
    else:
        st.info("ğŸ“ ì €ì¥ëœ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        st.write("AIê°€ ë¶„ì„ í›„ íŒŒì¼ì„ ì €ì¥í•©ë‹ˆë‹¤.")

client = None
if st.session_state.get("api_key"):
    try:
        client = OpenAI(api_key=st.session_state.get("api_key"))
    except Exception as e:
        st.error(f"âŒ OpenAI í´ë¼ì´ì–¸íŠ¸ ìƒì„± ì¤‘ ì˜¤ë¥˜: {str(e)}")
else:
    st.warning("â—ï¸ OpenAI API í‚¤ë¥¼ ì‚¬ì´ë“œë°”ì—ì„œ ì…ë ¥í•´ì£¼ì„¸ìš”.")

if "history" not in st.session_state:
    st.session_state.history = [{"role": "system", "content": SYSTEM_PROMPT}]
if "messages" not in st.session_state:
    st.session_state.messages = []
if not isinstance(st.session_state.messages, list):
    st.session_state.messages = []
st.session_state.messages = [
    m for m in st.session_state.messages 
    if m.get("content", "").strip() and m.get("role") in ["user", "assistant"]
]


for m in st.session_state.messages:
    content = m.get("content", "")
    if content and content.strip():
        with st.chat_message(m["role"]):
            if m["role"] == "assistant" and "progress_steps" in m:
                render_progress_steps(m["progress_steps"])
            st.markdown(content)


if "processing" not in st.session_state:
    st.session_state.processing = False

if prompt := st.chat_input("ì›¹ì—ì„œ ë¬´ì—‡ì„ ì°¾ì•„ë³¼ê¹Œìš”?", disabled=st.session_state.processing):
    st.session_state.messages.append({"role": "user", "content": prompt})
    st.session_state.processing = True
    st.rerun()

if st.session_state.processing and len(st.session_state.messages) > 0 and st.session_state.messages[-1]["role"] == "user":
    user_prompt = st.session_state.messages[-1]["content"]
    with st.chat_message("assistant"):
        if client is None:
            error_message = "âŒ OpenAI API í‚¤ê°€ í•„ìš”í•©ë‹ˆë‹¤. ì‚¬ì´ë“œë°”ì—ì„œ API í‚¤ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”."
            st.session_state.messages.append({"role": "assistant", "content": error_message})
            st.session_state.processing = False
            st.rerun()
        else:
            progress_placeholder = st.empty()
            try:
                # API í˜¸ì¶œ ì‹œ temperature ì „ë‹¬ ì¶”ê°€
                st.session_state.history, answer = research_assistant_streaming(
                    client, user_prompt, st.session_state.history, progress_placeholder, model_name
                )
                
                final_message = {
                    "role": "assistant", 
                    "content": answer, 
                    "progress_steps": st.session_state.get("progress_steps", [])
                }
                st.session_state.messages.append(final_message)
                
                st.session_state.processing = False
                st.session_state.progress_steps = []
                st.rerun()

            except Exception as e:
                progress_placeholder.empty()
                error_message = f"âŒ ì—°êµ¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}\n\nğŸ’¡ API í‚¤ì™€ ëª¨ë¸ ì„¤ì •ì„ í™•ì¸í•´ì£¼ì„¸ìš”."
                st.session_state.messages.append({"role": "assistant", "content": error_message})
                st.session_state.processing = False
                st.rerun()


if st.sidebar.button("ğŸ—‘ï¸ ëŒ€í™” ì´ˆê¸°í™”", disabled=st.session_state.get("processing", False)):
    for key in list(st.session_state.keys()):
        del st.session_state[key]
    st.session_state.history = [{"role": "system", "content": SYSTEM_PROMPT}]
    st.session_state.messages = []
    st.session_state.progress_steps = []
    st.success("âœ… ëŒ€í™”ê°€ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤!")
    st.rerun()
